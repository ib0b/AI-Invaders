{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing custom game environment\n",
    "from game import GameEnv\n",
    "#imporrt relevant  python libraries\n",
    "import math\n",
    "import random\n",
    "import pygame\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model,Sequential,load_model\n",
    "from keras.layers import Input, Dense ,Conv2D,Flatten,MaxPooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hides game window during training.Showing window slows down training/testing considerably\n",
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateShape (102, 4)\n",
      "stacked 4\n"
     ]
    }
   ],
   "source": [
    "#initialize environemnt\n",
    "env = GameEnv()\n",
    "\n",
    "stateSize = len(env.reset())\n",
    "stack_size =4 # Number of frames stacked\n",
    "\n",
    "#actions\n",
    "# 0 for no action\n",
    "# 1 for go left\n",
    "# 2 for go right\n",
    "# 3 for shoot\n",
    "action_size = 4 # 4 possible actions\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((stateSize), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "frameResize =  (40,30)    \n",
    " \n",
    "def stack_frames(stacked_frames, frame, is_new_episode):\n",
    "    #Stacks state data, based on number of frames - for including temporal information in training of the neural network \n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros(frameResize, dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for i in range(stack_size):\n",
    "          stacked_frames.append(frame)         \n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=1)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=1) \n",
    "    # stacked_state = stacked_state.reshape((80,60,4))\n",
    "    return stacked_state, stacked_frames\n",
    "testState = env.reset() \n",
    "lastState,stacked_frames  = stack_frames(stacked_frames, testState, True)\n",
    "print(\"stateShape\",lastState.shape)\n",
    "print(\"stacked\",len(stacked_frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self,actionCnt):\n",
    "        self.stateCnt = lastState.shape\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        self.model_ = self._createModel()  # target network\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten( input_shape=self.stateCnt))\n",
    "        model.add(Dense(512,activation='relu'))    \n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(512, activation='relu'))  \n",
    "        model.add(Dense(512, activation='relu'))      \n",
    "        model.add(Dense(units=self.actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=LEARNING_RATE)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.00001))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "    def loadModel(self,file):\n",
    "        self.model = load_model(file)\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s[np.newaxis,:], target).flatten()\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "MEMORY_CAPACITY = 200000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "EXPLORATION_STOP = 500000   # at this step epsilon will be 0.01\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STOP  # speed of decay\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, actionCnt):       \n",
    "        self.actionCnt = actionCnt\n",
    "        self.brain = Brain(actionCnt)\n",
    "#         self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "#         if random.random() < self.epsilon:\n",
    "#             return random.randint(0, self.actionCnt-1)\n",
    "#         else:\n",
    "        return np.argmax(self.brain.predictOne(s))\n",
    "    def load(self,file):\n",
    "        self.brain.loadModel(file)\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        x, y, errors = self._getTargets([(0, sample)])\n",
    "        self.memory.add(errors[0], sample)\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def _getTargets(self, batch):\n",
    "        no_state = np.zeros(lastState.shape)\n",
    "\n",
    "        states = np.array([ o[1][0] for o in batch ])\n",
    "        states_ = np.array([ (no_state if o[1][3] is None else o[1][3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "\n",
    "        p_ = agent.brain.predict(states_, target=False)\n",
    "        pTarget_ = agent.brain.predict(states_, target=True)\n",
    "\n",
    "        x = np.zeros((len(batch), *lastState.shape))\n",
    "        y = np.zeros((len(batch), self.actionCnt))\n",
    "        errors = np.zeros(len(batch))\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            oldVal = t[a]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * pTarget_[i][ np.argmax(p_[i]) ]  # double DQN\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        x, y, errors = self._getTargets(batch)\n",
    "\n",
    "        #update errors\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.brain.train(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 408)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               209408    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 999,428\n",
      "Trainable params: 999,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 408)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               209408    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 999,428\n",
      "Trainable params: 999,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create intelligent agent and assign 4 as the action space\n",
    "agent = Agent(4)\n",
    "#load trained neural network weights\n",
    "agent.load(\"trained.h5\")\n",
    "#prints out neural network architecture summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"dqn\")\n",
    "agent.steps = 0\n",
    "#agen runs\n",
    "invasionScores =[]\n",
    "winScores =[]\n",
    "deadScores = []\n",
    "score_history=[]\n",
    "winsHistory = []\n",
    "stepsTracker = []\n",
    "deads = []\n",
    "hist = []\n",
    "wins = 0\n",
    "lastWins=0\n",
    "env = GameEnv(1/60,graphics=True)\n",
    "for i in range(5000):\n",
    "    done = False\n",
    "    #reset environment and get first frame\n",
    "    frame = env.reset()\n",
    "    frame,t1,t2,t3 =env.step(0)\n",
    "    #set custom rewards\n",
    "    # env.bullets=90\n",
    "    \n",
    "    actions = []        \n",
    "    #set initial state\n",
    "    s ,stacked_frames= stack_frames(stacked_frames, frame, True)\n",
    "    R = 0\n",
    "    startTime = time.time()\n",
    "    while not done:\n",
    "        \n",
    "        a = agent.act(s)\n",
    "        actions.append(a)\n",
    "        #simulate action\n",
    "        frame, r, done, win = env.step(a)\n",
    "        ns ,stacked_frames= stack_frames(stacked_frames, frame, True)\n",
    "        if(done):\n",
    "            ns = None\n",
    "\n",
    "       \n",
    "        R+=r\n",
    "        #set nextstate as now the current state\n",
    "        s=ns\n",
    "    #metrics\n",
    "    score=R\n",
    "    steps=env.steps\n",
    "    if(win==1):\n",
    "      wins+=win\n",
    "      winScores.append(R)\n",
    "      lastWins+=abs(win)\n",
    "    if(win==-2):\n",
    "      deadScores.append(R)\n",
    "    if(win==-1):\n",
    "      invasionScores.append(R)\n",
    "    if(win!=0):\n",
    "      #invasion\n",
    "      hist.append(win)\n",
    "    dur = time.time()-startTime\n",
    "    \n",
    "    winsHistory.append(wins)\n",
    "    stepsTracker.append(steps)  \n",
    "    score_history.append(score)   \n",
    "    deads.append(env.enemyBlock.numDead)\n",
    "    \n",
    "    #graphs\n",
    "    if(i%50==0):\n",
    "      lastWins = 0\n",
    "    \n",
    "  \n",
    "    graphLimit = 50\n",
    "  \n",
    "    print(f\"episode {i} score {score} steps={steps} wins={wins} numDead={env.enemyBlock.numDead} percent={wins/(i+1)} last50 ={lastWins/(i%50+1)} avg score { np.mean(score_history[max(0, i-100):(i+1)])} time {dur} \")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "    fig, (row1,row2) = plt.subplots(2,5,figsize=(16,6))\n",
    "\n",
    "\n",
    "    if len(winsHistory)<=graphLimit:\n",
    "        row1[1].plot(winsHistory)\n",
    "        row1[1].plot([j for j in range(0,len(winsHistory))])\n",
    "\n",
    "    else:\n",
    "        row1[1].plot([k for k in range(i-graphLimit, i)],winsHistory[-graphLimit:],color=\"r\")  \n",
    "        row1[1].plot([k for k in range(i-graphLimit, i)],[j for j in range(winsHistory[-graphLimit],winsHistory[-graphLimit]+graphLimit)],color=\"b\")\n",
    "    row1[1].set_xlabel('win history(50)', fontsize=14)\n",
    "\n",
    "    row1[0].plot([j for j in range(max(i-graphLimit,0),max(i-graphLimit,0)+graphLimit)],[k for k in range(winsHistory[max(i-graphLimit,0)],winsHistory[max(i-graphLimit,0)]+graphLimit)])\n",
    "    row1[0].plot(winsHistory)\n",
    "    row1[0].set_xlabel('win history', fontsize=14)\n",
    "\n",
    "    row1[2].plot(score_history[-50:])\n",
    "    row1[2].set_xlabel('rewards (50)', fontsize=14)\n",
    "\n",
    "    row1[3].hist(hist,bins=[-2,-1,0,1])\n",
    "    row1[3].set_xlabel('end states', fontsize=14)\n",
    "\n",
    "    row1[4].hist(hist[-50:],bins=[-2,-1,0,1])\n",
    "    row1[4].set_xlabel('end states (50)', fontsize=14)\n",
    "\n",
    "    row2[0].hist(actions,bins=[0,1,2,3,4])\n",
    "    row2[0].set_xlabel('actions', fontsize=14)\n",
    "\n",
    "    row2[1].plot(stepsTracker)\n",
    "    row2[1].set_xlabel('steps', fontsize=14)\n",
    "\n",
    "    row2[2].plot(stepsTracker[-50:])\n",
    "    row2[2].set_xlabel('steps (50)', fontsize=14)\n",
    "\n",
    "    row2[3].plot(winScores,color=\"r\")\n",
    "    row2[3].plot(deadScores,color=\"blue\")\n",
    "    row2[3].plot(invasionScores,color=\"black\")\n",
    "    row2[3].set_xlabel('end state scores', fontsize=14)\n",
    "\n",
    "\n",
    "    row2[4].plot(deads,color=\"r\")\n",
    "    row2[4].set_xlabel('enemies killed', fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    if(i%1==0):\n",
    "      display.clear_output(wait=True)   \n",
    "      display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
